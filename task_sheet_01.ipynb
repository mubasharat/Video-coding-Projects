{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical task sheet 01 -- color space conversion, subsampling, chroma keying and noise\n",
    "\n",
    "The first practical task sheet will be about color conversion and chroma keying.\n",
    "For this please checkout the following links:\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Color_space\n",
    "* https://en.wikipedia.org/wiki/YCbCr\n",
    "* https://en.wikipedia.org/wiki/Chroma_subsampling\n",
    "* https://en.wikipedia.org/wiki/Chroma_key\n",
    "* https://en.wikipedia.org/wiki/Image_noise\n",
    "\n",
    "Transmission standards use different color spaces, due to the different end devices, however, cameras also use different color spaces to record videos, it is thus required to convert from one color space to a different one.\n",
    "We will tackle in this task sheet several parts of color space conversion and subsampling.\n",
    "\n",
    "Afterward, we will have a look at traditional chroma-keying, a technique used in e.g. television studios.\n",
    "The key idea of chroma keying is to replace the background with something else, e.g. a weather map or similar.\n",
    "\n",
    "Finally, we will check out how to reduce salt and pepper noise removal and edge detection for an image.\n",
    "\n",
    "**General Hint**: in each code cell the parts where code needs to be added are marked with TBD, prefer simple code than complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install requirements (this cell should not produce any errors, otherwise check dependencies and guide)\n",
    "!pip3 install --user numpy pandas matplotlib scipy jupyter scikit-image scikit-learn scikit-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (2.2.5)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.13/site-packages (3.10.1)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: scikit-image in ./venv/lib/python3.13/site-packages (0.25.2)\n",
      "Requirement already satisfied: sk-video in ./venv/lib/python3.13/site-packages (1.1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.13/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.13/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./venv/lib/python3.13/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in ./venv/lib/python3.13/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./venv/lib/python3.13/site-packages (from scikit-image) (2025.3.30)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./venv/lib/python3.13/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scipy scikit-learn scikit-image sk-video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions and required imports\n",
    "\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "\n",
    "def show_image(img):\n",
    "    \"\"\" shows an image (3d array) in a jupytor cell\"\"\"\n",
    "    skimage.io.imshow(img)\n",
    "    skimage.io.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1: Color space conversion\n",
    "The most commonly used color space for video processing is $YC_bC_r$, in the following cells we will manually implement such a conversion from digital RGB values.\n",
    "\n",
    "Important to know is that the conversion of $[0,1]$-scaled RGB to $YC_bC_r$ is done using the following equations (ITU-R BT.601 conversion):\n",
    "\n",
    "$$ Y = 16 + ( 65.481 \\cdot R + 128.553 \\cdot G + 24.966 \\cdot B) $$\n",
    "$$ C_b = 128 + (-37.797 \\cdot R - 74.203 \\cdot G + 112.0 \\cdot B) $$\n",
    "$$ C_r = 128 + (112.0  \\cdot R - 93.786 \\cdot G - 18.214 \\cdot B) $$\n",
    "\n",
    "After conversion the components are handled as 8-bit unsigned integer planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: '/Users/basharatnaeem/Documents/color_example.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# use method from skimage.io to read \"color_example.jpg\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m example_image = \u001b[43mskimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolor_example.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TBD\u001b[39;00m\n\u001b[32m      4\u001b[39m show_image(example_image)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# access each color channel, and convert to [0,1] scaled values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/skimage/_shared/utils.py:328\u001b[39m, in \u001b[36mdeprecate_parameter.__call__.<locals>.fixed_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.new_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# Assign old value to new one\u001b[39;00m\n\u001b[32m    326\u001b[39m         kwargs[\u001b[38;5;28mself\u001b[39m.new_name] = deprecated_value\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/skimage/io/_io.py:82\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(fname, as_gray, plugin, **plugin_args)\u001b[39m\n\u001b[32m     79\u001b[39m         plugin = \u001b[33m'\u001b[39m\u001b[33mtifffile\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname, _hide_plugin_deprecation_warnings():\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     img = \u001b[43mcall_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimread\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplugin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mplugin_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[33m'\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/skimage/_shared/utils.py:538\u001b[39m, in \u001b[36mdeprecate_func.__call__.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    536\u001b[39m stacklevel = \u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.get_stack_length(func) - stack_rank\n\u001b[32m    537\u001b[39m warnings.warn(message, category=\u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=stacklevel)\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/skimage/io/manage_plugins.py:254\u001b[39m, in \u001b[36mcall_plugin\u001b[39m\u001b[34m(kind, *args, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[32m    252\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCould not find the plugin \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/skimage/io/_plugins/imageio_plugin.py:11\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimread\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     out = np.asarray(\u001b[43mimageio_imread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out.flags[\u001b[33m'\u001b[39m\u001b[33mWRITEABLE\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     13\u001b[39m         out = out.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/imageio/v3.py:53\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m] = index\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mplugin_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(img_file.read(**call_kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/imageio/core/imopen.py:113\u001b[39m, in \u001b[36mimopen\u001b[39m\u001b[34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m     request.format_hint = format_hint\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     request = \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m source = \u001b[33m\"\u001b[39m\u001b[33m<bytes>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/imageio/core/request.py:249\u001b[39m, in \u001b[36mRequest.__init__\u001b[39m\u001b[34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[39m\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venv/lib/python3.13/site-packages/imageio/core/request.py:409\u001b[39m, in \u001b[36mRequest._parse_uri\u001b[39m\u001b[34m(self, uri)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_read_request:\n\u001b[32m    407\u001b[39m     \u001b[38;5;66;03m# Reading: check that the file exists (but is allowed a dir)\u001b[39;00m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(fn):\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo such file: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % fn)\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# Writing: check that the directory to write to does exist\u001b[39;00m\n\u001b[32m    412\u001b[39m     dn = os.path.dirname(fn)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file: '/Users/basharatnaeem/Documents/color_example.jpg'"
     ]
    }
   ],
   "source": [
    "# use method from skimage.io to read \"color_example.jpg\"\n",
    "example_image = skimage.io.imread(\"color_example.jpg\")  # TBD\n",
    "\n",
    "show_image(example_image)\n",
    "\n",
    "# access each color channel, and convert to [0,1] scaled values\n",
    "R = example_image[:, :, 0] / 255.0\n",
    "G = example_image[:, :, 1] / 255.0\n",
    "B = example_image[:, :, 2] / 255.0\n",
    "\n",
    "\n",
    "# show all channels\n",
    "show_image(R)\n",
    "show_image(G)\n",
    "show_image(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# just as a fun step, we create a new image (with same dimensions), and swap the channels to BGR\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#   important if you use the R,G,B values from the previous cell, rescale to [0,255] values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m channel_swap = \u001b[43mnp\u001b[49m.zeros(example_image.shape, dtype=np.uint8)\n\u001b[32m      5\u001b[39m channel_swap[:, :, \u001b[32m0\u001b[39m] = (B * \u001b[32m255\u001b[39m).astype(np.uint8)\n\u001b[32m      6\u001b[39m channel_swap[:, :, \u001b[32m1\u001b[39m] = (G * \u001b[32m255\u001b[39m).astype(np.uint8)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# just as a fun step, we create a new image (with same dimensions), and swap the channels to BGR\n",
    "#   important if you use the R,G,B values from the previous cell, rescale to [0,255] values\n",
    "\n",
    "channel_swap = np.zeros(example_image.shape, dtype=np.uint8)\n",
    "channel_swap[:, :, 0] = (B * 255).astype(np.uint8)\n",
    "channel_swap[:, :, 1] = (G * 255).astype(np.uint8)\n",
    "channel_swap[:, :, 2] = (R * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "# and we show the channel swapped image\n",
    "show_image(channel_swap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'R' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# lets now convert the R,G,B image to Y, C_b, C_r\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m Y  = \u001b[32m16\u001b[39m   + (\u001b[32m65.481\u001b[39m * \u001b[43mR\u001b[49m + \u001b[32m128.553\u001b[39m * G + \u001b[32m24.966\u001b[39m * B)\n\u001b[32m      3\u001b[39m Cb = \u001b[32m128\u001b[39m  + (-\u001b[32m37.797\u001b[39m * R - \u001b[32m74.203\u001b[39m * G + \u001b[32m112.0\u001b[39m * B)\n\u001b[32m      4\u001b[39m Cr = \u001b[32m128\u001b[39m  + (\u001b[32m112.0\u001b[39m * R - \u001b[32m93.786\u001b[39m * G - \u001b[32m18.214\u001b[39m * B)\n",
      "\u001b[31mNameError\u001b[39m: name 'R' is not defined"
     ]
    }
   ],
   "source": [
    "# lets now convert the R,G,B image to Y, C_b, C_r\n",
    "Y  = 16   + (65.481 * R + 128.553 * G + 24.966 * B)\n",
    "Cb = 128  + (-37.797 * R - 74.203 * G + 112.0 * B)\n",
    "Cr = 128  + (112.0 * R - 93.786 * G - 18.214 * B)\n",
    "\n",
    "\n",
    "\n",
    "# important convert type to uint8 \n",
    "Y = np.clip(Y, 0, 255).astype(np.uint8)\n",
    "Cb = np.clip(Cb, 0, 255).astype(np.uint8)\n",
    "Cr = np.clip(Cr, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "show_image(Y)\n",
    "show_image(Cb)\n",
    "show_image(Cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m combined\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# save the result\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m converted = rgb_to_y_cb_cr(\u001b[43mexample_image\u001b[49m)\n\u001b[32m     26\u001b[39m skimage.io.imsave(\u001b[33m\"\u001b[39m\u001b[33mycbcr.png\u001b[39m\u001b[33m\"\u001b[39m, converted)\n",
      "\u001b[31mNameError\u001b[39m: name 'example_image' is not defined"
     ]
    }
   ],
   "source": [
    "# put the steps before for RGB to Y C_b C_r conversion into one method\n",
    "def rgb_to_y_cb_cr(img_rgb):\n",
    "    \"\"\" method to convert a given RGB image to YC_bC_r according to the steps before,\n",
    "    \"\"\"\n",
    "    \n",
    "    R = img_rgb[:, :, 0] / 255.0\n",
    "    G = img_rgb[:, :, 1] / 255.0\n",
    "    B = img_rgb[:, :, 2] / 255.0\n",
    "\n",
    "    Y  = 16   + (65.481 * R + 128.553 * G + 24.966 * B)\n",
    "    Cb = 128  + (-37.797 * R - 74.203 * G + 112.0 * B)\n",
    "    Cr = 128  + (112.0 * R - 93.786 * G - 18.214 * B)\n",
    "\n",
    "    Y = np.clip(Y, 0, 255).astype(np.uint8)\n",
    "    Cb = np.clip(Cb, 0, 255).astype(np.uint8)\n",
    "    Cr = np.clip(Cr, 0, 255).astype(np.uint8)\n",
    "\n",
    "    combined = np.zeros(img_rgb.shape, dtype=np.uint8)\n",
    "    combined[:, :, 0] = Y\n",
    "    combined[:, :, 1] = Cb\n",
    "    combined[:, :, 2] = Cr\n",
    "    return combined\n",
    "# save the result\n",
    "\n",
    "converted = rgb_to_y_cb_cr(example_image)\n",
    "skimage.io.imsave(\"ycbcr.png\", converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2: 4:2:0 chroma subsampling\n",
    "After we are now able to convert RGB images to $YC_bC_r$, we can now implement chroma subsampling.\n",
    "The general idea here is that human perception is more sensitive to changes in luma than in color.\n",
    "We will handly 4:2:0 subsampling in this task, this results in the full resolution for Y and only half of the resolution (thus each second pixel) for each C component in the $YC_bC_r$ color space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "import skimage.color\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert example_image to  $YC_bC_r$ using skimage.color.rgb2yuv and perform 4:2:0 chroma subsampling\n",
    "# note: skimage.color.rgb2yuv is not the same as rgb_to_y_cb_cr, we assume here rgb2yuv \\approx ycbcr for demonstration\n",
    "\n",
    "# use method from skimage.io to read \"color_example.jpg\"\n",
    "example_image = skimage.io.imread(\"color_example.jpg\")   # TBD\n",
    "\n",
    "# convert example image to yuv space, we use this as ycbcr \n",
    "ycbcr = skimage.color.rgb2yuv(example_image)   \n",
    "\n",
    "# select separate components\n",
    "Y = ycbcr[:, :, 0]  #TBD\n",
    "C_b = ycbcr[:, :, 1]\n",
    "C_r = ycbcr[:, :, 2]\n",
    "\n",
    "# sub sample Cb, Cr components, according to 4:2:0 method:\n",
    "C_b_s =C_b[::2, ::2]    # TBD\n",
    "C_r_s = C_r[::2, ::2]   # TBD\n",
    "\n",
    "# show each component\n",
    "\n",
    "show_image(Y)\n",
    "show_image(C_b_s)\n",
    "show_image(C_r_s)\n",
    "\n",
    "# Y, C_b_s, and C_r_s are [0,1]-scaled float64 arrays\n",
    "# to store them we need to convert each component to a [0,255] integer scaled numpy array\n",
    "\n",
    "Y_uint8 = np.clip(Y * 255, 0, 255).astype(np.uint8)# TBD\n",
    "C_b_s_uint8 = np.clip(C_b_s * 255, 0, 255).astype(np.uint8)# TBD\n",
    "C_r_s_uint8 = np.clip(C_r_s * 255, 0, 255).astype(np.uint8)# TBD\n",
    "\n",
    "\n",
    "\n",
    "# save each component\n",
    "skimage.io.imsave(\"Y.png\", Y_uint8)\n",
    "skimage.io.imsave(\"C_b_2.png\", C_b_s_uint8)\n",
    "skimage.io.imsave(\"C_r_0.png\", C_r_s_uint8)\n",
    "\n",
    "assert((np.array(C_b_s.shape[0:2]) * 2 == example_image.shape[0:2]).all())\n",
    "assert((np.array(C_r_s.shape[0:2]) * 2 == example_image.shape[0:2]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything together in one method\n",
    "def chroma_subsampling_4_2_0(img_rgb):\n",
    "    \"\"\" returns each component with applied 4:2:0 sampling\"\"\"\n",
    "    ycbcr = skimage.color.rgb2yuv(img_rgb)# TBD start\n",
    "    Y = ycbcr[:, :, 0]# ...\n",
    "    C_b_s = ycbcr[:, :, 1][::2, ::2]# TBD end\n",
    "    C_r_s = ycbcr[:, :, 2][::2, ::2]\n",
    "    return np.array([Y, C_b_s, C_r_s], dtype=object)\n",
    "\n",
    "res_sub_sampling = chroma_subsampling_4_2_0(example_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 3: Chroma Keying\n",
    "Assuming you have a well-illuminated scene, with a static colored background (usually blue or green background colors are used). \n",
    "The idea is to use a setup consisting of a camera (where parameters like FOV/focal length, camera position are captured), a blue/green box, lights, and some animated/replacement for the background.\n",
    "Using such a recorded scene will end up, e.g., the following example image, background, and combined version.\n",
    "(it should be mentioned that the image here is just an example and was processed before to make this task feasible)\n",
    "\n",
    "<img src=\"chroma_foreground.jpg\" alt=\"chroma_foreground\" style=\"width: 25%;float: left;\"/>\n",
    "<img src=\"chroma_background.jpg\" alt=\"chroma_background\" style=\"width: 25%;float: left;\"/> \n",
    "<img src=\"chroma_combined.jpg\" alt=\"chroma_combined\" style=\"width: 25%;float: left;\"/>\n",
    "\n",
    "<div style=\" clear: both;\"></div>\n",
    "\n",
    "We will handle this task automatically using python, here we assume a static scene, moreover, in a real setup there are more things to be considered (camera position, changing background, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: read the foreground and background images\n",
    "# with scikit-image skimage.io function \n",
    "# (check for the suitable way)\n",
    "\n",
    "# read the foreground image\n",
    "foreground = skimage.io.imread(\"chroma_foreground.jpg\")   # TBD\n",
    "# show foreground image\n",
    "show_image(foreground)\n",
    "\n",
    "# read background in a similar manner and show it\n",
    "background = skimage.io.imread(\"chroma_background.jpg\")    # TBD\n",
    "# show background image\n",
    "show_image(background) \n",
    "\n",
    "# check if both images have same shape\n",
    "assert(foreground.shape == background.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined lower blue threshold\n",
    "lower_threshold = np.array([0, 0, 100])    # [R, G, B] values\n",
    "\n",
    "# defined upper blue threshold\n",
    "upper_threshold =np.array([100, 100, 255])    # TBD\n",
    "\n",
    " \n",
    "def threshold_mask(image, lower_threshold, upper_threshold):\n",
    "    # for each color channel create a mask based on the defined thresholds\n",
    "    r_mask = ((image[:,:, 0] >= lower_threshold[0]) & (image[:,:, 0] <= upper_threshold[0]))\n",
    "    g_mask = ((image[:,:, 1] >= lower_threshold[1]) & (image[:,:, 1] <= upper_threshold[1]))  # TBD\n",
    "    b_mask = ((image[:,:, 2] >= lower_threshold[2]) & (image[:,:, 2] <= upper_threshold[2])) # TBD\n",
    "    # combine the channel masks\n",
    "    mask_value = r_mask & g_mask & b_mask\n",
    "    return mask_value.copy().astype('uint8')\n",
    "\n",
    "# create a mask based on a lower and upper threshold\n",
    "foreground_mask = threshold_mask(foreground, lower_threshold, upper_threshold)\n",
    "\n",
    "# show the final foreground mask\n",
    "show_image(foreground_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine foreground with mask and add background\n",
    "bg = background.copy()  # copy background image\n",
    "\n",
    "# for background set the not masked values zero \n",
    "bg[foreground_mask != True] = [0, 0, 0]\n",
    "\n",
    "# copy foreground\n",
    "fg = foreground.copy()  # TBD \n",
    "# for the forground set the masked values to zero\n",
    "fg[foreground_mask == 0] = [0, 0, 0]# TBO\n",
    "\n",
    "# show both masked images\n",
    "show_image(bg)\n",
    "show_image(fg)\n",
    "\n",
    "# combine fg and bg image\n",
    "combined_image = bg + fg  # TBD\n",
    "show_image(combined_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything in one method\n",
    "\n",
    "def threshold_chroma(foreground, background, lower_threshold, upper_threshold):  # TBD\n",
    "    \"\"\" one method without showing any image, to perform the above implemented steps in one go, \n",
    "        * use threshold_mask that was defined before\n",
    "    \"\"\"\n",
    "    foreground_mask = threshold_mask(foreground, lower_threshold, upper_threshold)\n",
    "\n",
    "    bg = background.copy()# TBD start\n",
    "    bg[foreground_mask != 1] = [0, 0, 0]# ...\n",
    "    fg = foreground.copy() \n",
    "    fg[foreground_mask == 0] = [0, 0, 0] \n",
    "    combined_image = bg + fg # TBD end\n",
    "    return combined_image\n",
    "\n",
    "chroma_res_img = threshold_chroma(foreground, background, lower_threshold, upper_threshold)\n",
    "show_image(chroma_res_img)\n",
    "\n",
    "# save resulting combined image\n",
    "skimage.io.imsave(\"chroma_result.jpg\", chroma_res_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 4: Salt and pepper noise\n",
    "This type of noise is also known as impulse noise, here some white or black pixels occur randomly in an image, removing them will help to improve the image quality, and also for later post-processing the noise is usually not required.\n",
    "They usually originate from dead pixels inside the camera, thus they can also occur in all three channels (colored salt and pepper noise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the noise_example.jpg\n",
    "from skimage.io import imread\n",
    "example = imread('noise_example.jpg') # TBD\n",
    "show_image(example)\n",
    "# because this image does not have noise, we will include now some noise\n",
    "\n",
    "def add_salt_pepper_noise(img, SNR):\n",
    "    img_with_noise = img.copy()\n",
    "    mask = np.random.choice((0, 1, 2), size=img.shape, p=[SNR, (1 - SNR) / 2., (1 - SNR) / 2.])\n",
    "    img_with_noise[mask == 1] = 255 # salt noise\n",
    "    img_with_noise[mask == 2] = 0 # pepper\n",
    "\n",
    "    return img_with_noise\n",
    "\n",
    "noise_example = add_salt_pepper_noise(example, 0.8)\n",
    "\n",
    "show_image(noise_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we do the median filtering using a 2D convolution, \n",
    "#   we develop a method to perform 2D convolutions\n",
    "\n",
    "def convolve_2d(image, kernel_size=(3,3)):\n",
    "    \"\"\" yields (view, i, j) of a 2D convolution of \n",
    "            the 2D input image \n",
    "    \n",
    "        for simplicity the borders are ignored\n",
    "    \"\"\"\n",
    "    keh,kew = kernel_size\n",
    "    h, w = image.shape[0:2]  # it should work for gray input images, as well as for colored \n",
    "    for i in range(h - keh + 1): # TBD\n",
    "        for j in range( w - kew + 1): # TBD\n",
    "            # view is the current \"kernel\" wide view of the image in the convolution\n",
    "            view = image[i:i+keh, j:j+kew] # TBD\n",
    "            \n",
    "            # yield is similar to \"return\", however a local state of this iteration is stored\n",
    "            # thus multiple times calling this method will produce each steps of the loops inside \n",
    "            # see https://www.geeksforgeeks.org/use-yield-keyword-instead-return-keyword-python/\n",
    "            yield view, i, j\n",
    "\n",
    "res = list(convolve_2d(\n",
    "        np.array([\n",
    "            [11,12,13,14],\n",
    "            [21,22,23,24],\n",
    "            [31,32,33,34],\n",
    "            [41,42,43,44]\n",
    "        ]),\n",
    "        kernel_size = (3,3)\n",
    "    )\n",
    ")\n",
    "\n",
    "assert(len(res) == 4)\n",
    "assert([x[0].sum() for x in res] == [198, 207, 288, 297])\n",
    "assert((res[0][0] == [[11, 12, 13], [21, 22, 23], [31, 32, 33]]).all())\n",
    "assert((res[1][0] == [[12, 13, 14], [22, 23, 24], [32, 33, 34]]).all())\n",
    "assert((res[2][0] == [[21, 22, 23], [31, 32, 33], [41, 42, 43]]).all())\n",
    "assert((res[3][0] == [[22, 23, 24], [32, 33, 34], [42, 43, 44]]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step is to remove the introduced salt and pepper noise,\n",
    "# a convolution with a median filter for each color component is one possible approach\n",
    "# here an own implementation is required\n",
    "\n",
    "def remove_salt_pepper_noise(img_rgb, kernel=(3,3)):\n",
    "    \"\"\" remove colored salt and pepper noise using a convolutional kernel with a default size of 3x3 and \n",
    "        median filtering per channel,\n",
    "        \n",
    "        use the convolve_2d method inside, \n",
    "        for simplicity the borders are ignored\n",
    "    \"\"\"\n",
    "    cleaned_img = img_rgb.copy()\n",
    "    keh,kew = kernel\n",
    "    \n",
    "    for c in range(3):# TBD start\n",
    "        chanel = img_rgb[:, :, c]\n",
    "        new_chanel = chanel.copy()# ...\n",
    "        for view, i, j in convolve_2d(chanel, kernel):\n",
    "            new_chanel[i + keh//2, j + kew//2] = np.median(view)\n",
    "        cleaned_img[:, :, c] = new_chanel # TBD end\n",
    "    return cleaned_img\n",
    "\n",
    "show_image(remove_salt_pepper_noise(noise_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to detect edges we now perform a 2D convolution with a laplace filter kernel\n",
    "\n",
    "def edge_detection_laplace(img_rgb):\n",
    "    \"\"\" detect edgeds using a laplace filter,\n",
    "        here a 2D convolution is performed with a laplace filter kernel\n",
    "        \n",
    "        only the luminance channel of the input image is used,\n",
    "        use the convolve_2d method inside, \n",
    "        for simplicity borders are ignored\n",
    "    \"\"\"\n",
    "    kernel = np.array([\n",
    "        [0,  0, 0],  # TBD\n",
    "        [1, -3, 1],  # TBD\n",
    "        [0,  1, 0]   # TBD\n",
    "    ]\n",
    "    )\n",
    "    # we only apply the laplace filter to the Y channel\n",
    "    Y = skimage.color.rgb2yuv(img_rgb)[:,:,0]\n",
    "    laplacian_edges = np.zeros(Y.shape)\n",
    "    for view, i, j in convolve_2d(Y, kernel.shape):# TBD start\n",
    "        laplacian_edges[i + 2, j + 2] = np.sum(view * kernel)\n",
    "    \n",
    "    \n",
    "    return laplacian_edges# TBD end\n",
    "\n",
    "# we use the same example image as before, and \n",
    "#    detect edges using the laplace filter kernel\n",
    "show_image(edge_detection_laplace(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
